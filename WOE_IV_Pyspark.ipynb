{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/websitecreatr99/Pyspark_WOE_IV/blob/main/WOE_IV_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV8eQKjXg8wP",
        "outputId": "4ba56259-7be3-4147-94fa-783e04c2666c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.17 2022-10-18\n",
            "OpenJDK Runtime Environment (build 11.0.17+8-post-Ubuntu-1ubuntu220.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.17+8-post-Ubuntu-1ubuntu220.04, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "!java --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz0jnIXvhmLO",
        "outputId": "e3d51686-ed3f-4ddc-e40d-6ae4f2b23b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark[pandas_on_spark]==3.2.1\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.8/dist-packages (from pyspark[pandas_on_spark]==3.2.1) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pyspark[pandas_on_spark]==3.2.1) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.8/dist-packages (from pyspark[pandas_on_spark]==3.2.1) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23.2->pyspark[pandas_on_spark]==3.2.1) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23.2->pyspark[pandas_on_spark]==3.2.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.2->pyspark[pandas_on_spark]==3.2.1) (1.15.0)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853643 sha256=0da1a0bb5fe5b2fd7cd88246a173ff6dee35cb294472ca78df59f3a37b510a2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/94/83/915c9059e4b038e2d43a6058f307fe1c3e8536e5745f3b23b7\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark[pandas_on_spark]==3.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r11uJk1ohpKL"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2si8Hre8htA4"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "               .appName('SparkByExamples.com') \\\n",
        "               .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Qk06TvPZhwZm"
      },
      "outputs": [],
      "source": [
        "data = spark.read.csv(\"/content/Grid-Table-table694.csv\", header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YzblZXuiMV_",
        "outputId": "6c63f212-c864-4e63-8d7c-11dcf100c90b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "type(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2ZSjpDyiREa",
        "outputId": "5681fdfd-7cc4-4883-e162-92cc658ece53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----+-----+-----+----+----+-------+\n",
            "|capsule|age|race|dpros|dcaps| psa| vol|gleason|\n",
            "+-------+---+----+-----+-----+----+----+-------+\n",
            "|      0| 80|   1|    2|    1| 1.4|   0|      6|\n",
            "|      0| 72|   1|    3|    2| 6.7|   0|      7|\n",
            "|      0| 70|   1|    1|    2| 4.9|   0|      6|\n",
            "|      0| 76|   2|    2|    1|51.2|  20|      7|\n",
            "|      0| 69|   1|    1|    1|12.3|55.9|      6|\n",
            "|      1| 71|   1|    3|    2| 3.3|   0|      8|\n",
            "|      0| 68|   2|    4|    2|31.9|   0|      7|\n",
            "|      0| 61|   2|    4|    2|66.7|27.2|      7|\n",
            "|      0| 69|   1|    1|    1| 3.9|  24|      7|\n",
            "|      0| 68|   2|    1|    2|  13|   0|      6|\n",
            "|      1| 68|   2|    4|    2|   4|   0|      7|\n",
            "|      1| 72|   1|    2|    2|21.2|   0|      7|\n",
            "|      1| 72|   1|    4|    2|22.7|   0|      9|\n",
            "|      1| 65|   1|    4|    2|  39|   0|      7|\n",
            "|      0| 75|   1|    1|    1| 7.5|   0|      5|\n",
            "|      0| 73|   1|    2|    1| 2.6|   0|      5|\n",
            "|      0| 75|   2|    1|    1| 2.5|   0|      5|\n",
            "|      0| 70|   1|    2|    1| 2.6|11.8|      5|\n",
            "|      0| 54|   1|    1|    2| 2.8|   0|      6|\n",
            "|      1| 67|   2|    3|    2| 8.6|25.5|      7|\n",
            "+-------+---+----+-----+-----+----+----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "hPbor8QPiV16"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "class WOE_IV(object):\n",
        "    def __init__(self, df: DataFrame, cols_to_woe: [str], label_column: str, good_label: str):\n",
        "        self.df = df\n",
        "        self.cols_to_woe = cols_to_woe\n",
        "        self.label_column = label_column\n",
        "        self.good_label = good_label\n",
        "        self.fit_data = {}\n",
        "\n",
        "    def fit(self):\n",
        "        for col_to_woe in self.cols_to_woe:\n",
        "            total_good = self.compute_total_amount_of_good()\n",
        "            total_bad = self.compute_total_amount_of_bad()\n",
        "            \n",
        "            woe_df = self.df.select(col_to_woe)\n",
        "            categories = woe_df.distinct().collect()\n",
        "            for category_row in categories:\n",
        "                category = category_row[col_to_woe]\n",
        "                good_amount = self.compute_good_amount(col_to_woe, category)\n",
        "                bad_amount = self.compute_bad_amount(col_to_woe, category)\n",
        "\n",
        "                good_amount = good_amount if good_amount != 0 else 0.5\n",
        "                bad_amount = bad_amount if bad_amount != 0 else 0.5\n",
        "\n",
        "                good_dist = good_amount / total_good\n",
        "                bad_dist = bad_amount / total_bad\n",
        "\n",
        "                self.build_fit_data(col_to_woe, category, good_dist, bad_dist)\n",
        "\n",
        "    def transform(self, df: DataFrame):\n",
        "        def _encode_woe(col_to_woe_):\n",
        "            return F.coalesce(\n",
        "                *[F.when(F.col(col_to_woe_) == category, F.lit(woe_iv['woe']))\n",
        "                  for category, woe_iv in self.fit_data[col_to_woe_].items()]\n",
        "            )\n",
        "\n",
        "        for col_to_woe, woe_info in self.fit_data.items():\n",
        "            df = df.withColumn(col_to_woe + '_woe', _encode_woe(col_to_woe))\n",
        "        return df\n",
        "\n",
        "    def compute_total_amount_of_good(self):\n",
        "        return self.df.select(self.label_column).filter(F.col(self.label_column) == self.good_label).count()\n",
        "\n",
        "    def compute_total_amount_of_bad(self):\n",
        "        return self.df.select(self.label_column).filter(F.col(self.label_column) != self.good_label).count()\n",
        "\n",
        "    def compute_good_amount(self, col_to_woe: str, category: str):\n",
        "        return self.df.select(col_to_woe, self.label_column)\\\n",
        "                      .filter(\n",
        "                            (F.col(col_to_woe) == category) & (F.col(self.label_column) == self.good_label)\n",
        "                      ).count()\n",
        "\n",
        "    def compute_bad_amount(self, col_to_woe: str, category: str):\n",
        "        return self.df.select(col_to_woe, self.label_column)\\\n",
        "                      .filter(\n",
        "                            (F.col(col_to_woe) == category) & (F.col(self.label_column) != self.good_label)\n",
        "                      ).count()\n",
        "\n",
        "    def build_fit_data(self, col_to_woe, category, good_dist, bad_dist):\n",
        "        woe_info = {\n",
        "            category: {\n",
        "                'woe': math.log(good_dist / bad_dist),\n",
        "                'iv':  math.log(good_dist / bad_dist) * (good_dist - bad_dist)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if col_to_woe not in self.fit_data:\n",
        "            self.fit_data[col_to_woe] = woe_info\n",
        "        else:\n",
        "            self.fit_data[col_to_woe].update(woe_info)\n",
        "\n",
        "    def compute_iv(self):\n",
        "        iv_dict = {}\n",
        "\n",
        "        for woe_col, categories in self.fit_data.items():\n",
        "            iv_dict[woe_col] = 0\n",
        "            for category, woe_iv in categories.items():\n",
        "                iv_dict[woe_col] += woe_iv['iv']\n",
        "        return iv_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OKLkbohQisTB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# from woe import WOE_IV\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # spark = SparkSession.builder.appName('woe-encoding').getOrCreate()\n",
        "\n",
        "    df = spark.read.csv(\"/content/Grid-Table-table694.csv\", header=True)\n",
        "\n",
        "    # cols_to_woe = ['col_a', 'col_b']\n",
        "    cols_to_woe = ['capsule','age','race','dpros','dcaps', 'psa', 'vol','gleason']\n",
        "    woe = WOE_IV(df, cols_to_woe, 'capsule', 0)\n",
        "\n",
        "    # woe encoding\n",
        "    woe.fit()\n",
        "    encoded_df = woe.transform(df)\n",
        "\n",
        "    # information value\n",
        "    ivs = woe.compute_iv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VWhiSGljL8P",
        "outputId": "cd673072-61a1-4971-98e5-e3b8f7ec1eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'capsule': 11.813946865670689, 'age': 0.2646040114629034, 'race': 0.0097655015227033, 'dpros': 0.46969877049741027, 'dcaps': 0.24883754658468282, 'psa': 1.0633445755222402, 'vol': 0.41973615975490813, 'gleason': 1.1571232573722214}\n"
          ]
        }
      ],
      "source": [
        "print(ivs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA4ZdKIpTF0d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGqUXZz5/IGuLMRpelnQ3P",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}